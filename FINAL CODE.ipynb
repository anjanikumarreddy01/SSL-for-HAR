{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KbHk7erUbptM",
    "outputId": "c1aea835-0989-49a2-a241-46c9e1339a19"
   },
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio\n",
    "!pip install scipy pandas scikit-learn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "0wtxp2nIb1Ts"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "from glob import glob\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import scipy.io\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "cacw_Rcfb5aB"
   },
   "outputs": [],
   "source": [
    "class HAR_Dataset(Dataset):\n",
    "    def __init__(self, X, y=None, window_size=128):\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.y = None if y is None else torch.LongTensor(y)\n",
    "        self.window_size = window_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Ensure the sequence length is correct\n",
    "        x = self.X[idx]\n",
    "        if x.shape[1] > self.window_size:\n",
    "            # If sequence is too long, take a random window\n",
    "            start = np.random.randint(0, x.shape[1] - self.window_size)\n",
    "            x = x[:, start:start + self.window_size]\n",
    "        elif x.shape[1] < self.window_size:\n",
    "            # If sequence is too short, pad with zeros\n",
    "            padding = torch.zeros(x.shape[0], self.window_size - x.shape[1])\n",
    "            x = torch.cat([x, padding], dim=1)\n",
    "\n",
    "        if self.y is None:\n",
    "            return x\n",
    "        return x, self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "kKxs4fW_b7eo"
   },
   "outputs": [],
   "source": [
    "def load_mobiact_data(base_path, window_size=128):\n",
    "    print(\"Loading MobiAct dataset...\")\n",
    "    all_data = []\n",
    "    all_labels = []\n",
    "\n",
    "    for activity_folder in glob(os.path.join(base_path, \"*\")):\n",
    "        if os.path.isdir(activity_folder):\n",
    "            activity_name = os.path.basename(activity_folder)\n",
    "\n",
    "            csv_files = glob(os.path.join(activity_folder, \"*.csv\"))\n",
    "            if not csv_files:\n",
    "                continue\n",
    "\n",
    "            for data_file in csv_files:\n",
    "                try:\n",
    "                    # Read CSV with header handling\n",
    "                    data = pd.read_csv(data_file, header=0)  # Changed to header=0\n",
    "                    sensor_data = data.iloc[:, 1:4].values.astype(np.float32).T  # Skip timestamp column\n",
    "                    \n",
    "                    # Process to fixed window_size\n",
    "                    time_steps = sensor_data.shape[1]\n",
    "                    if time_steps > window_size:\n",
    "                        sensor_data = sensor_data[:, :window_size]\n",
    "                    elif time_steps < window_size:\n",
    "                        pad_width = ((0, 0), (0, window_size - time_steps))\n",
    "                        sensor_data = np.pad(sensor_data, pad_width, mode='constant', constant_values=0)\n",
    "                    \n",
    "                    all_data.append(sensor_data)\n",
    "                    all_labels.append(activity_name)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading file {data_file}: {str(e)}\")\n",
    "\n",
    "    # Convert to properly typed numpy array\n",
    "    X = np.array(all_data, dtype=np.float32)\n",
    "    unique_labels = sorted(set(all_labels))\n",
    "    label_to_id = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "    y = np.array([label_to_id[label] for label in all_labels], dtype=np.int64)\n",
    "\n",
    "    print(f\"Loaded {len(X)} samples with {len(unique_labels)} activities\")\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "o7GgQIbqb_wp"
   },
   "outputs": [],
   "source": [
    "def load_uci_har_data(base_path):\n",
    "    print(\"Loading UCI-HAR dataset...\")\n",
    "\n",
    "    def load_signals(folder_path, data_type='train'):\n",
    "        signals = []\n",
    "        # Load accelerometer data\n",
    "        for signal_type in ['body_acc_x', 'body_acc_y', 'body_acc_z']:\n",
    "            signal_path = os.path.join(folder_path, f'Inertial Signals/{signal_type}_{data_type}.txt')\n",
    "            if os.path.exists(signal_path):\n",
    "                signal = pd.read_csv(signal_path, header=None, delim_whitespace=True).values\n",
    "                signals.append(signal)\n",
    "            else:\n",
    "                print(f\"Warning: {signal_path} does not exist.\")\n",
    "                return None  # Return None if a file is missing\n",
    "        return np.array(signals)\n",
    "\n",
    "    # Load training data\n",
    "    train_path = os.path.join(base_path, 'train')\n",
    "    X_train = load_signals(train_path, data_type='train')\n",
    "    y_train = pd.read_csv(os.path.join(train_path, 'y_train.txt'), header=None).values.ravel() if X_train is not None else None\n",
    "\n",
    "    # Load test data\n",
    "    test_path = os.path.join(base_path, 'test')\n",
    "    X_test = load_signals(test_path, data_type='test')\n",
    "    y_test = pd.read_csv(os.path.join(test_path, 'y_test.txt'), header=None).values.ravel() if X_test is not None else None\n",
    "\n",
    "    # Check if either X_train or X_test is None\n",
    "    if X_train is None and X_test is None:\n",
    "        raise ValueError(\"Both training and testing data failed to load.\")\n",
    "\n",
    "    # Handle the case where X_train is None\n",
    "    if X_train is None:\n",
    "        X = np.transpose(X_test, (1, 0, 2))\n",
    "        y = y_test - 1\n",
    "        print(\"Only test data loaded.\")\n",
    "    elif X_test is None:\n",
    "        X = np.transpose(X_train, (1, 0, 2))\n",
    "        y = y_train - 1\n",
    "        print(\"Only train data loaded.\")\n",
    "    else:\n",
    "        # Combine train and test\n",
    "        X = np.concatenate([X_train, X_test], axis=1)  # Shape: (3, n_samples, time_steps)\n",
    "        X = np.transpose(X, (1, 0, 2))  # Shape: (n_samples, 3, time_steps)\n",
    "        y = np.concatenate([y_train, y_test]) - 1  # Make labels 0-based\n",
    "        print(\"Both train and test data loaded.\")\n",
    "\n",
    "    print(f\"Loaded {len(X)} samples\")\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "-AkgGgu4cGS7"
   },
   "outputs": [],
   "source": [
    "def load_usc_had_data(base_path, window_size=128):  # Added window_size parameter\n",
    "    print(\"Loading USC-HAD dataset...\")\n",
    "    all_data = []\n",
    "    all_labels = []\n",
    "\n",
    "    for subject_folder in sorted(glob(os.path.join(base_path, \"Subject*\"))):\n",
    "        subject_id = os.path.basename(subject_folder)\n",
    "        \n",
    "        for data_file in glob(os.path.join(subject_folder, \"*.mat\")):\n",
    "            try:\n",
    "                # Load .mat file\n",
    "                mat_data = scipy.io.loadmat(data_file)\n",
    "                sensor_readings = mat_data['sensor_readings']\n",
    "                \n",
    "                # CORRECTED: Extract first 3 sensors (columns) and transpose to (3, N)\n",
    "                acc_data = sensor_readings[:, :3].T  # Fixed slicing and transposed\n",
    "                \n",
    "                # CORRECTED: Standardize to window_size\n",
    "                time_steps = acc_data.shape[1]\n",
    "                if time_steps > window_size:\n",
    "                    # Random truncation\n",
    "                    start = np.random.randint(0, time_steps - window_size)\n",
    "                    acc_data = acc_data[:, start:start + window_size]\n",
    "                elif time_steps < window_size:\n",
    "                    # Zero-padding\n",
    "                    pad_width = ((0, 0), (0, window_size - time_steps))\n",
    "                    acc_data = np.pad(acc_data, pad_width, mode='constant')\n",
    "                \n",
    "                all_data.append(acc_data)\n",
    "                all_labels.append(int(os.path.basename(data_file)[1:].split('t')[0]) - 1)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {data_file}: {str(e)}\")\n",
    "\n",
    "    X = np.array(all_data, dtype=np.float32)\n",
    "    y = np.array(all_labels, dtype=np.int64)\n",
    "    print(f\"Loaded {len(X)} samples with window_size={window_size}\")\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "wXit9qLecKZI"
   },
   "outputs": [],
   "source": [
    "class CNN_GRN_Transformer(nn.Module):\n",
    "    def __init__(self, input_channels=3, seq_length=128, num_classes=12):\n",
    "        super(CNN_GRN_Transformer, self).__init__()\n",
    "\n",
    "        # Parameters\n",
    "        self.hidden_dim = 256\n",
    "\n",
    "        # CNN layers\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv1d(input_channels, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        # Calculate sequence length after CNN\n",
    "        self.seq_length_after_cnn = seq_length // 2\n",
    "\n",
    "        # GRN (Gated Recurrent Network)\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=128,\n",
    "            hidden_size=self.hidden_dim // 2,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "        # Transformer\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=self.hidden_dim,\n",
    "            nhead=8,\n",
    "            dim_feedforward=1024,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
    "\n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.hidden_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        # CNN feature extraction\n",
    "        x = self.cnn(x)\n",
    "\n",
    "        # Prepare for GRU (batch_size, seq_len, features)\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        # GRU processing\n",
    "        x, _ = self.gru(x)\n",
    "\n",
    "        # Transformer processing\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        # Global average pooling\n",
    "        x = torch.mean(x, dim=1)\n",
    "\n",
    "        # Classification\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "DRMB1wBJcNuL"
   },
   "outputs": [],
   "source": [
    "class CCLoss(nn.Module):\n",
    "    def __init__(self, temperature=0.07):\n",
    "        super(CCLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, features):\n",
    "        # Normalize features\n",
    "        features = nn.functional.normalize(features, dim=1)\n",
    "\n",
    "        # Compute similarity matrix\n",
    "        similarity = torch.mm(features, features.t()) / self.temperature\n",
    "\n",
    "        # Remove diagonal elements\n",
    "        mask = torch.eye(similarity.shape[0], device=similarity.device)\n",
    "        similarity = similarity * (1 - mask)\n",
    "\n",
    "        # Compute contrastive loss\n",
    "        exp_sim = torch.exp(similarity)\n",
    "        loss = -torch.log(exp_sim.sum(dim=1) / (exp_sim.sum() - exp_sim.diag()))\n",
    "\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "WIe1w7S7cPi1"
   },
   "outputs": [],
   "source": [
    "def pretrain(model, train_loader, device, epochs=50):\n",
    "    print(\"Starting pretraining phase...\")\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    cc_loss = CCLoss()\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch_idx, data in enumerate(train_loader):\n",
    "            data = data.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            features = model(data)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = cc_loss(features)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{epochs}] Batch [{batch_idx}/{len(train_loader)}] Loss: {loss.item():.4f}')\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f'Epoch [{epoch+1}/{epochs}] Average Loss: {avg_loss:.4f}')\n",
    "\n",
    "    return model\n",
    "def finetune(model, train_loader, val_loader, device, epochs=30):\n",
    "    print(\"Starting fine-tuning phase...\")\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5)\n",
    "\n",
    "    best_val_acc = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{epochs}] Batch [{batch_idx}/{len(train_loader)}] Loss: {loss.item():.4f}')\n",
    "\n",
    "        train_acc = 100 * correct / total\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += targets.size(0)\n",
    "                correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        val_acc = 100 * correct / total\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{epochs}]')\n",
    "        print(f'Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%')\n",
    "        print(f'Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%')\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            print(f'Saved new best model with validation accuracy: {val_acc:.2f}%')\n",
    "\n",
    "    # Add return values for validation predictions\n",
    "    all_val_preds = []\n",
    "    all_val_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            all_val_preds.extend(preds.cpu().numpy())\n",
    "            all_val_labels.extend(targets.cpu().numpy())\n",
    "    \n",
    "    return model, val_loss  \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_dataset(model, X, y, name, device):  # Add device parameter\n",
    "    dataset = HAR_Dataset(X, y, 128)\n",
    "    loader = DataLoader(dataset, batch_size=32)\n",
    "    \n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)  # Now has device\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(targets.cpu().numpy())\n",
    "    \n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    \n",
    "    print(f\"\\n{name} Dataset Evaluation:\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "   # print(classification_report(all_labels, all_preds, target_names=class_names)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "aUf4CK3ncUwM"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Device configuration\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Parameters\n",
    "    WINDOW_SIZE = 128\n",
    "    BATCH_SIZE = 32\n",
    "    NUM_CLASSES = 12  # Adjust based on your combined dataset\n",
    "\n",
    "    # Dataset paths - you'll need to set these based on your local setup\n",
    "    MOBIACT_PATH = r\"C:\\Users\\ASUS\\Desktop\\CSSHAR\\MobiAct_Dataset_v2.0\\MobiAct_Dataset_v2.0\\Annotated Data\"\n",
    "    UCI_HAR_PATH = r\"C:\\Users\\ASUS\\Desktop\\CSSHAR\\human+activity+recognition+using+smartphones\\UCI HAR Dataset\\UCI HAR Dataset\"\n",
    "    USC_HAD_PATH = r\"C:\\Users\\ASUS\\Desktop\\CSSHAR\\USC-HAD\\USC-HAD\"\n",
    "\n",
    "    # Load datasets\n",
    "    print(\"\\nLoading datasets...\")\n",
    "\n",
    "    # MobiAct for pretraining\n",
    "    X_mobiact, y_mobiact = load_mobiact_data(MOBIACT_PATH, window_size=WINDOW_SIZE)\n",
    "\n",
    "    # Check if MobiAct dataset is empty\n",
    "    if len(X_mobiact) == 0:\n",
    "        raise ValueError(\"MobiAct dataset is empty. Please check your data loading process.\")\n",
    "    print(\"mobiact done\")\n",
    "    # Load other datasets for fine-tuning\n",
    "    X_uci, y_uci = load_uci_har_data(UCI_HAR_PATH)\n",
    "    print(\"UCI done\")\n",
    "\n",
    "    # Load USC-HAD dataset\n",
    "    X_usc, y_usc = load_usc_had_data(USC_HAD_PATH, window_size=WINDOW_SIZE)\n",
    "    \n",
    "\n",
    "    # Check if USC-HAD dataset is empty\n",
    "    if len(X_usc) == 0:\n",
    "        raise ValueError(\"USC-HAD dataset is empty. Please check your data loading process.\")\n",
    "    print(\"USC done\")\n",
    "    # Create model\n",
    "    model = CNN_GRN_Transformer(\n",
    "        input_channels=3,\n",
    "        seq_length=WINDOW_SIZE,\n",
    "        num_classes=NUM_CLASSES\n",
    "    ).to(device)\n",
    "    print(\"model created\")\n",
    "    \n",
    "    \n",
    "    # Pretraining phase (using only MobiAct)\n",
    "    pretrain_dataset = HAR_Dataset(X_mobiact, window_size=WINDOW_SIZE)\n",
    "    pretrain_loader = DataLoader(pretrain_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    model = pretrain(model, pretrain_loader, device, epochs=50)\n",
    "\n",
    "    # Save pretrained model\n",
    "    torch.save(model.state_dict(), 'pretrained_model.pth')\n",
    "    print(\"Saved pretrained model\")\n",
    "\n",
    "    # Fine-tuning phase (using UCI and USC datasets)\n",
    "    print(\"\\nStarting fine-tuning on combined datasets...\")\n",
    "    \n",
    "    # Combine UCI and USC datasets\n",
    "    X_combined = np.concatenate([X_uci, X_usc])\n",
    "    y_combined = np.concatenate([y_uci, y_usc])\n",
    "    \n",
    "    # Split into train/val sets (80/20)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_combined, y_combined, test_size=0.2, random_state=42, stratify=y_combined\n",
    "    )\n",
    "    \n",
    "    # Create datasets and loaders\n",
    "    train_dataset = HAR_Dataset(X_train, y_train, WINDOW_SIZE)\n",
    "    val_dataset = HAR_Dataset(X_val, y_val, WINDOW_SIZE)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "    # Fine-tune the model\n",
    "     # During fine-tuning\n",
    "    model, training_losses = finetune(model, train_loader, val_loader, device, epochs=40)\n",
    "    \n",
    "    \n",
    "    # Final evaluation\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(targets.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    print(f\"\\nTOTAL Final Evaluation Results:\")\n",
    "    print(f\"Mean F1 Score: {f1:.4f}\")\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    # Evaluate on both datasets separately\n",
    "    evaluate_dataset(model, X_uci, y_uci, \"UCI-HAR\", device)  # Pass device\n",
    "    evaluate_dataset(model, X_usc, y_usc, \"USC-HAD\", device)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KEcuo2_GcZGF",
    "outputId": "b306b675-de7b-4897-a122-2284a35344d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Loading datasets...\n",
      "Loading MobiAct dataset...\n",
      "Loaded 3294 samples with 20 activities\n",
      "mobiact done\n",
      "Loading UCI-HAR dataset...\n",
      "Both train and test data loaded.\n",
      "Loaded 10299 samples\n",
      "UCI done\n",
      "Loading USC-HAD dataset...\n",
      "Loaded 840 samples with window_size=128\n",
      "USC done\n",
      "model created\n",
      "Starting pretraining phase...\n",
      "Epoch [1/50] Batch [0/103] Loss: 4.2304\n",
      "Epoch [1/50] Batch [100/103] Loss: 3.4669\n",
      "Epoch [1/50] Average Loss: 3.4822\n",
      "Epoch [2/50] Batch [0/103] Loss: 3.4669\n",
      "Epoch [2/50] Batch [100/103] Loss: 3.4661\n",
      "Epoch [2/50] Average Loss: 3.4662\n",
      "Epoch [3/50] Batch [0/103] Loss: 3.4664\n",
      "Epoch [3/50] Batch [100/103] Loss: 3.4659\n",
      "Epoch [3/50] Average Loss: 3.4656\n",
      "Epoch [4/50] Batch [0/103] Loss: 3.4661\n",
      "Epoch [4/50] Batch [100/103] Loss: 3.4658\n",
      "Epoch [4/50] Average Loss: 3.4653\n",
      "Epoch [5/50] Batch [0/103] Loss: 3.4658\n",
      "Epoch [5/50] Batch [100/103] Loss: 3.4658\n",
      "Epoch [5/50] Average Loss: 3.4652\n",
      "Epoch [6/50] Batch [0/103] Loss: 3.4658\n",
      "Epoch [6/50] Batch [100/103] Loss: 3.4658\n",
      "Epoch [6/50] Average Loss: 3.4651\n",
      "Epoch [7/50] Batch [0/103] Loss: 3.4658\n",
      "Epoch [7/50] Batch [100/103] Loss: 3.4657\n",
      "Epoch [7/50] Average Loss: 3.4651\n",
      "Epoch [8/50] Batch [0/103] Loss: 3.4657\n",
      "Epoch [8/50] Batch [100/103] Loss: 3.4658\n",
      "Epoch [8/50] Average Loss: 3.4653\n",
      "Epoch [9/50] Batch [0/103] Loss: 3.4659\n",
      "Epoch [9/50] Batch [100/103] Loss: 3.4658\n",
      "Epoch [9/50] Average Loss: 3.4652\n",
      "Epoch [10/50] Batch [0/103] Loss: 3.4658\n",
      "Epoch [10/50] Batch [100/103] Loss: 3.4658\n",
      "Epoch [10/50] Average Loss: 3.4651\n",
      "Epoch [11/50] Batch [0/103] Loss: 3.4657\n",
      "Epoch [11/50] Batch [100/103] Loss: 3.4657\n",
      "Epoch [11/50] Average Loss: 3.4651\n",
      "Epoch [12/50] Batch [0/103] Loss: 3.4657\n",
      "Epoch [12/50] Batch [100/103] Loss: 3.4657\n",
      "Epoch [12/50] Average Loss: 3.4651\n",
      "Epoch [13/50] Batch [0/103] Loss: 3.4657\n",
      "Epoch [13/50] Batch [100/103] Loss: 3.4657\n",
      "Epoch [13/50] Average Loss: 3.4652\n",
      "Epoch [14/50] Batch [0/103] Loss: 3.4658\n",
      "Epoch [14/50] Batch [100/103] Loss: 3.4657\n",
      "Epoch [14/50] Average Loss: 3.4651\n",
      "Epoch [15/50] Batch [0/103] Loss: 3.4657\n",
      "Epoch [15/50] Batch [100/103] Loss: 3.4657\n",
      "Epoch [15/50] Average Loss: 3.4651\n",
      "Epoch [16/50] Batch [0/103] Loss: 3.4657\n",
      "Epoch [16/50] Batch [100/103] Loss: 3.4657\n",
      "Epoch [16/50] Average Loss: 3.4651\n",
      "Epoch [17/50] Batch [0/103] Loss: 3.4657\n",
      "Epoch [17/50] Batch [100/103] Loss: 3.4657\n",
      "Epoch [17/50] Average Loss: 3.4651\n",
      "Epoch [18/50] Batch [0/103] Loss: 3.4657\n",
      "Epoch [18/50] Batch [100/103] Loss: 3.4657\n",
      "Epoch [18/50] Average Loss: 3.4651\n",
      "Epoch [19/50] Batch [0/103] Loss: 3.4657\n",
      "Epoch [19/50] Batch [100/103] Loss: 3.4657\n",
      "Epoch [19/50] Average Loss: 3.4651\n",
      "Epoch [20/50] Batch [0/103] Loss: 3.4657\n",
      "Epoch [20/50] Batch [100/103] Loss: 3.4657\n",
      "Epoch [20/50] Average Loss: 3.4651\n",
      "Epoch [21/50] Batch [0/103] Loss: 3.4657\n",
      "Epoch [21/50] Batch [100/103] Loss: 3.4657\n",
      "Epoch [21/50] Average Loss: 3.4651\n",
      "Epoch [22/50] Batch [0/103] Loss: 3.4657\n",
      "Epoch [22/50] Batch [100/103] Loss: 3.4657\n",
      "Epoch [22/50] Average Loss: 3.4651\n",
      "Epoch [23/50] Batch [0/103] Loss: 3.4657\n",
      "Epoch [23/50] Batch [100/103] Loss: 3.4657\n",
      "Epoch [23/50] Average Loss: 3.4651\n",
      "Epoch [24/50] Batch [0/103] Loss: 3.4657\n",
      "Epoch [24/50] Batch [100/103] Loss: 3.4657\n",
      "Epoch [24/50] Average Loss: 3.4651\n",
      "Epoch [25/50] Batch [0/103] Loss: 3.4657\n",
      "Epoch [25/50] Batch [100/103] Loss: 3.4657\n",
      "Epoch [25/50] Average Loss: 3.4651\n",
      "Epoch [26/50] Batch [0/103] Loss: 3.4657\n",
      "Epoch [26/50] Batch [100/103] Loss: 3.4657\n",
      "Epoch [26/50] Average Loss: 3.4651\n",
      "Epoch [27/50] Batch [0/103] Loss: 3.4657\n",
      "Epoch [27/50] Batch [100/103] Loss: 3.4657\n",
      "Epoch [27/50] Average Loss: 3.4651\n",
      "Epoch [28/50] Batch [0/103] Loss: 3.4657\n",
      "Epoch [28/50] Batch [100/103] Loss: 3.4657\n",
      "Epoch [28/50] Average Loss: 3.4651\n",
      "Epoch [29/50] Batch [0/103] Loss: 3.4657\n",
      "Epoch [29/50] Batch [100/103] Loss: 3.4657\n",
      "Epoch [29/50] Average Loss: 3.4651\n",
      "Epoch [30/50] Batch [0/103] Loss: 3.4657\n",
      "Epoch [30/50] Batch [100/103] Loss: 3.4657\n",
      "Epoch [30/50] Average Loss: 3.4651\n",
      "Epoch [31/50] Batch [0/103] Loss: 3.4657\n",
      "Epoch [31/50] Batch [100/103] Loss: 3.4657\n",
      "Epoch [31/50] Average Loss: 3.4651\n",
      "Epoch [32/50] Batch [0/103] Loss: 3.4657\n",
      "Epoch [32/50] Batch [100/103] Loss: 3.4657\n",
      "Epoch [32/50] Average Loss: 3.4651\n",
      "Epoch [33/50] Batch [0/103] Loss: 3.4657\n",
      "Epoch [33/50] Batch [100/103] Loss: 3.4657\n",
      "Epoch [33/50] Average Loss: 3.4651\n",
      "Epoch [34/50] Batch [0/103] Loss: 3.4657\n",
      "Epoch [34/50] Batch [100/103] Loss: 3.4657\n",
      "Epoch [34/50] Average Loss: 3.4651\n",
      "Epoch [35/50] Batch [0/103] Loss: 3.4657\n",
      "Epoch [35/50] Batch [100/103] Loss: 3.4657\n",
      "Epoch [35/50] Average Loss: 3.4651\n",
      "Epoch [36/50] Batch [0/103] Loss: 3.4657\n",
      "Epoch [36/50] Batch [100/103] Loss: 3.4657\n",
      "Epoch [36/50] Average Loss: 3.4651\n",
      "Epoch [37/50] Batch [0/103] Loss: 3.4657\n",
      "Epoch [37/50] Batch [100/103] Loss: 3.4657\n",
      "Epoch [37/50] Average Loss: 3.4651\n",
      "Epoch [38/50] Batch [0/103] Loss: 3.4657\n",
      "Epoch [38/50] Batch [100/103] Loss: 3.4657\n",
      "Epoch [38/50] Average Loss: 3.4651\n",
      "Epoch [39/50] Batch [0/103] Loss: 3.4657\n",
      "Epoch [39/50] Batch [100/103] Loss: 3.4657\n",
      "Epoch [39/50] Average Loss: 3.4651\n",
      "Epoch [40/50] Batch [0/103] Loss: 3.4657\n",
      "Epoch [40/50] Batch [100/103] Loss: 3.4657\n",
      "Epoch [40/50] Average Loss: 3.4651\n",
      "Epoch [41/50] Batch [0/103] Loss: 3.4657\n",
      "Epoch [41/50] Batch [100/103] Loss: 3.4657\n",
      "Epoch [41/50] Average Loss: 3.4651\n",
      "Epoch [42/50] Batch [0/103] Loss: 3.4657\n",
      "Epoch [42/50] Batch [100/103] Loss: 3.4657\n",
      "Epoch [42/50] Average Loss: 3.4651\n",
      "Epoch [43/50] Batch [0/103] Loss: 3.4657\n",
      "Epoch [43/50] Batch [100/103] Loss: 3.4657\n",
      "Epoch [43/50] Average Loss: 3.4651\n",
      "Epoch [44/50] Batch [0/103] Loss: 3.4657\n",
      "Epoch [44/50] Batch [100/103] Loss: 3.4657\n",
      "Epoch [44/50] Average Loss: 3.4651\n",
      "Epoch [45/50] Batch [0/103] Loss: 3.4657\n",
      "Epoch [45/50] Batch [100/103] Loss: 3.4657\n",
      "Epoch [45/50] Average Loss: 3.4651\n",
      "Epoch [46/50] Batch [0/103] Loss: 3.4657\n",
      "Epoch [46/50] Batch [100/103] Loss: 3.4657\n",
      "Epoch [46/50] Average Loss: 3.4651\n",
      "Epoch [47/50] Batch [0/103] Loss: 3.4657\n",
      "Epoch [47/50] Batch [100/103] Loss: 3.4657\n",
      "Epoch [47/50] Average Loss: 3.4651\n",
      "Epoch [48/50] Batch [0/103] Loss: 3.4657\n",
      "Epoch [48/50] Batch [100/103] Loss: 3.4657\n",
      "Epoch [48/50] Average Loss: 3.4651\n",
      "Epoch [49/50] Batch [0/103] Loss: 3.4657\n",
      "Epoch [49/50] Batch [100/103] Loss: 3.4657\n",
      "Epoch [49/50] Average Loss: 3.4651\n",
      "Epoch [50/50] Batch [0/103] Loss: 3.4657\n",
      "Epoch [50/50] Batch [100/103] Loss: 3.4657\n",
      "Epoch [50/50] Average Loss: 3.4651\n",
      "Saved pretrained model\n",
      "\n",
      "Starting fine-tuning on combined datasets...\n",
      "Starting fine-tuning phase...\n",
      "Epoch [1/40] Batch [0/279] Loss: 6.7944\n",
      "Epoch [1/40] Batch [100/279] Loss: 1.3741\n",
      "Epoch [1/40] Batch [200/279] Loss: 1.0584\n",
      "Epoch [1/40]\n",
      "Train Loss: 1.6840 | Train Acc: 35.19%\n",
      "Val Loss: 0.9326 | Val Acc: 55.75%\n",
      "Saved new best model with validation accuracy: 55.75%\n",
      "Epoch [2/40] Batch [0/279] Loss: 0.8765\n",
      "Epoch [2/40] Batch [100/279] Loss: 0.7591\n",
      "Epoch [2/40] Batch [200/279] Loss: 0.7450\n",
      "Epoch [2/40]\n",
      "Train Loss: 0.8639 | Train Acc: 57.09%\n",
      "Val Loss: 0.7371 | Val Acc: 60.82%\n",
      "Saved new best model with validation accuracy: 60.82%\n",
      "Epoch [3/40] Batch [0/279] Loss: 0.9524\n",
      "Epoch [3/40] Batch [100/279] Loss: 0.5977\n",
      "Epoch [3/40] Batch [200/279] Loss: 0.7710\n",
      "Epoch [3/40]\n",
      "Train Loss: 0.7567 | Train Acc: 60.68%\n",
      "Val Loss: 0.6807 | Val Acc: 63.33%\n",
      "Saved new best model with validation accuracy: 63.33%\n",
      "Epoch [4/40] Batch [0/279] Loss: 0.8638\n",
      "Epoch [4/40] Batch [100/279] Loss: 0.8342\n",
      "Epoch [4/40] Batch [200/279] Loss: 0.7591\n",
      "Epoch [4/40]\n",
      "Train Loss: 0.7053 | Train Acc: 63.17%\n",
      "Val Loss: 0.6488 | Val Acc: 64.99%\n",
      "Saved new best model with validation accuracy: 64.99%\n",
      "Epoch [5/40] Batch [0/279] Loss: 0.6081\n",
      "Epoch [5/40] Batch [100/279] Loss: 0.8233\n",
      "Epoch [5/40] Batch [200/279] Loss: 0.5647\n",
      "Epoch [5/40]\n",
      "Train Loss: 0.6755 | Train Acc: 65.11%\n",
      "Val Loss: 0.6274 | Val Acc: 69.34%\n",
      "Saved new best model with validation accuracy: 69.34%\n",
      "Epoch [6/40] Batch [0/279] Loss: 0.6898\n",
      "Epoch [6/40] Batch [100/279] Loss: 0.5690\n",
      "Epoch [6/40] Batch [200/279] Loss: 0.5048\n",
      "Epoch [6/40]\n",
      "Train Loss: 0.6077 | Train Acc: 70.32%\n",
      "Val Loss: 0.4929 | Val Acc: 78.41%\n",
      "Saved new best model with validation accuracy: 78.41%\n",
      "Epoch [7/40] Batch [0/279] Loss: 0.4689\n",
      "Epoch [7/40] Batch [100/279] Loss: 0.6173\n",
      "Epoch [7/40] Batch [200/279] Loss: 0.5741\n",
      "Epoch [7/40]\n",
      "Train Loss: 0.5222 | Train Acc: 75.12%\n",
      "Val Loss: 0.4494 | Val Acc: 79.94%\n",
      "Saved new best model with validation accuracy: 79.94%\n",
      "Epoch [8/40] Batch [0/279] Loss: 0.4905\n",
      "Epoch [8/40] Batch [100/279] Loss: 0.6192\n",
      "Epoch [8/40] Batch [200/279] Loss: 0.4968\n",
      "Epoch [8/40]\n",
      "Train Loss: 0.4744 | Train Acc: 77.81%\n",
      "Val Loss: 0.4282 | Val Acc: 81.28%\n",
      "Saved new best model with validation accuracy: 81.28%\n",
      "Epoch [9/40] Batch [0/279] Loss: 0.2508\n",
      "Epoch [9/40] Batch [100/279] Loss: 0.4010\n",
      "Epoch [9/40] Batch [200/279] Loss: 0.2740\n",
      "Epoch [9/40]\n",
      "Train Loss: 0.4641 | Train Acc: 78.33%\n",
      "Val Loss: 0.3990 | Val Acc: 82.23%\n",
      "Saved new best model with validation accuracy: 82.23%\n",
      "Epoch [10/40] Batch [0/279] Loss: 0.5201\n",
      "Epoch [10/40] Batch [100/279] Loss: 0.4462\n",
      "Epoch [10/40] Batch [200/279] Loss: 0.4132\n",
      "Epoch [10/40]\n",
      "Train Loss: 0.4414 | Train Acc: 79.96%\n",
      "Val Loss: 0.4131 | Val Acc: 80.88%\n",
      "Epoch [11/40] Batch [0/279] Loss: 0.4525\n",
      "Epoch [11/40] Batch [100/279] Loss: 0.7178\n",
      "Epoch [11/40] Batch [200/279] Loss: 0.3835\n",
      "Epoch [11/40]\n",
      "Train Loss: 0.4267 | Train Acc: 80.62%\n",
      "Val Loss: 0.3975 | Val Acc: 83.03%\n",
      "Saved new best model with validation accuracy: 83.03%\n",
      "Epoch [12/40] Batch [0/279] Loss: 0.2769\n",
      "Epoch [12/40] Batch [100/279] Loss: 0.3016\n",
      "Epoch [12/40] Batch [200/279] Loss: 0.3302\n",
      "Epoch [12/40]\n",
      "Train Loss: 0.4061 | Train Acc: 81.95%\n",
      "Val Loss: 0.3852 | Val Acc: 83.84%\n",
      "Saved new best model with validation accuracy: 83.84%\n",
      "Epoch [13/40] Batch [0/279] Loss: 0.4844\n",
      "Epoch [13/40] Batch [100/279] Loss: 0.4553\n",
      "Epoch [13/40] Batch [200/279] Loss: 0.5313\n",
      "Epoch [13/40]\n",
      "Train Loss: 0.3924 | Train Acc: 82.75%\n",
      "Val Loss: 0.3624 | Val Acc: 84.16%\n",
      "Saved new best model with validation accuracy: 84.16%\n",
      "Epoch [14/40] Batch [0/279] Loss: 0.4666\n",
      "Epoch [14/40] Batch [100/279] Loss: 0.2695\n",
      "Epoch [14/40] Batch [200/279] Loss: 0.3574\n",
      "Epoch [14/40]\n",
      "Train Loss: 0.3933 | Train Acc: 82.61%\n",
      "Val Loss: 0.3559 | Val Acc: 84.02%\n",
      "Epoch [15/40] Batch [0/279] Loss: 0.2323\n",
      "Epoch [15/40] Batch [100/279] Loss: 0.2562\n",
      "Epoch [15/40] Batch [200/279] Loss: 0.4029\n",
      "Epoch [15/40]\n",
      "Train Loss: 0.3754 | Train Acc: 83.36%\n",
      "Val Loss: 0.3622 | Val Acc: 83.66%\n",
      "Epoch [16/40] Batch [0/279] Loss: 0.2788\n",
      "Epoch [16/40] Batch [100/279] Loss: 0.2922\n",
      "Epoch [16/40] Batch [200/279] Loss: 0.3561\n",
      "Epoch [16/40]\n",
      "Train Loss: 0.3684 | Train Acc: 83.91%\n",
      "Val Loss: 0.3591 | Val Acc: 83.62%\n",
      "Epoch [17/40] Batch [0/279] Loss: 0.3371\n",
      "Epoch [17/40] Batch [100/279] Loss: 0.6108\n",
      "Epoch [17/40] Batch [200/279] Loss: 0.4160\n",
      "Epoch [17/40]\n",
      "Train Loss: 0.3553 | Train Acc: 84.31%\n",
      "Val Loss: 0.3698 | Val Acc: 83.08%\n",
      "Epoch [18/40] Batch [0/279] Loss: 0.5822\n",
      "Epoch [18/40] Batch [100/279] Loss: 0.3117\n",
      "Epoch [18/40] Batch [200/279] Loss: 0.2767\n",
      "Epoch [18/40]\n",
      "Train Loss: 0.3526 | Train Acc: 84.51%\n",
      "Val Loss: 0.3415 | Val Acc: 84.47%\n",
      "Saved new best model with validation accuracy: 84.47%\n",
      "Epoch [19/40] Batch [0/279] Loss: 0.2158\n",
      "Epoch [19/40] Batch [100/279] Loss: 0.2416\n",
      "Epoch [19/40] Batch [200/279] Loss: 0.5488\n",
      "Epoch [19/40]\n",
      "Train Loss: 0.3457 | Train Acc: 84.75%\n",
      "Val Loss: 0.3376 | Val Acc: 85.28%\n",
      "Saved new best model with validation accuracy: 85.28%\n",
      "Epoch [20/40] Batch [0/279] Loss: 0.2387\n",
      "Epoch [20/40] Batch [100/279] Loss: 0.2534\n",
      "Epoch [20/40] Batch [200/279] Loss: 0.2055\n",
      "Epoch [20/40]\n",
      "Train Loss: 0.3380 | Train Acc: 85.34%\n",
      "Val Loss: 0.3457 | Val Acc: 84.34%\n",
      "Epoch [21/40] Batch [0/279] Loss: 0.2286\n",
      "Epoch [21/40] Batch [100/279] Loss: 0.3563\n",
      "Epoch [21/40] Batch [200/279] Loss: 0.3042\n",
      "Epoch [21/40]\n",
      "Train Loss: 0.3401 | Train Acc: 85.36%\n",
      "Val Loss: 0.3412 | Val Acc: 84.07%\n",
      "Epoch [22/40] Batch [0/279] Loss: 0.4786\n",
      "Epoch [22/40] Batch [100/279] Loss: 0.3184\n",
      "Epoch [22/40] Batch [200/279] Loss: 0.3077\n",
      "Epoch [22/40]\n",
      "Train Loss: 0.3297 | Train Acc: 85.57%\n",
      "Val Loss: 0.3408 | Val Acc: 85.10%\n",
      "Epoch [23/40] Batch [0/279] Loss: 0.3184\n",
      "Epoch [23/40] Batch [100/279] Loss: 0.3942\n",
      "Epoch [23/40] Batch [200/279] Loss: 0.3949\n",
      "Epoch [23/40]\n",
      "Train Loss: 0.3180 | Train Acc: 85.80%\n",
      "Val Loss: 0.3563 | Val Acc: 83.12%\n",
      "Epoch [24/40] Batch [0/279] Loss: 0.4577\n",
      "Epoch [24/40] Batch [100/279] Loss: 0.3504\n",
      "Epoch [24/40] Batch [200/279] Loss: 0.2602\n",
      "Epoch [24/40]\n",
      "Train Loss: 0.3201 | Train Acc: 86.04%\n",
      "Val Loss: 0.3216 | Val Acc: 86.45%\n",
      "Saved new best model with validation accuracy: 86.45%\n",
      "Epoch [25/40] Batch [0/279] Loss: 0.3843\n",
      "Epoch [25/40] Batch [100/279] Loss: 0.1878\n",
      "Epoch [25/40] Batch [200/279] Loss: 0.2546\n",
      "Epoch [25/40]\n",
      "Train Loss: 0.3198 | Train Acc: 86.37%\n",
      "Val Loss: 0.3275 | Val Acc: 85.14%\n",
      "Epoch [26/40] Batch [0/279] Loss: 0.2590\n",
      "Epoch [26/40] Batch [100/279] Loss: 0.2539\n",
      "Epoch [26/40] Batch [200/279] Loss: 0.2405\n",
      "Epoch [26/40]\n",
      "Train Loss: 0.3106 | Train Acc: 86.38%\n",
      "Val Loss: 0.3178 | Val Acc: 85.73%\n",
      "Epoch [27/40] Batch [0/279] Loss: 0.5230\n",
      "Epoch [27/40] Batch [100/279] Loss: 0.2923\n",
      "Epoch [27/40] Batch [200/279] Loss: 0.1242\n",
      "Epoch [27/40]\n",
      "Train Loss: 0.3028 | Train Acc: 86.79%\n",
      "Val Loss: 0.3189 | Val Acc: 86.00%\n",
      "Epoch [28/40] Batch [0/279] Loss: 0.2917\n",
      "Epoch [28/40] Batch [100/279] Loss: 0.3588\n",
      "Epoch [28/40] Batch [200/279] Loss: 0.2360\n",
      "Epoch [28/40]\n",
      "Train Loss: 0.3049 | Train Acc: 86.76%\n",
      "Val Loss: 0.3193 | Val Acc: 85.59%\n",
      "Epoch [29/40] Batch [0/279] Loss: 0.2565\n",
      "Epoch [29/40] Batch [100/279] Loss: 0.2044\n",
      "Epoch [29/40] Batch [200/279] Loss: 0.2028\n",
      "Epoch [29/40]\n",
      "Train Loss: 0.3005 | Train Acc: 86.96%\n",
      "Val Loss: 0.3231 | Val Acc: 86.09%\n",
      "Epoch [30/40] Batch [0/279] Loss: 0.4037\n",
      "Epoch [30/40] Batch [100/279] Loss: 0.2577\n",
      "Epoch [30/40] Batch [200/279] Loss: 0.2841\n",
      "Epoch [30/40]\n",
      "Train Loss: 0.2962 | Train Acc: 86.98%\n",
      "Val Loss: 0.3255 | Val Acc: 85.46%\n",
      "Epoch [31/40] Batch [0/279] Loss: 0.2852\n",
      "Epoch [31/40] Batch [100/279] Loss: 0.3708\n",
      "Epoch [31/40] Batch [200/279] Loss: 0.1827\n",
      "Epoch [31/40]\n",
      "Train Loss: 0.2928 | Train Acc: 87.39%\n",
      "Val Loss: 0.2978 | Val Acc: 86.54%\n",
      "Saved new best model with validation accuracy: 86.54%\n",
      "Epoch [32/40] Batch [0/279] Loss: 0.1110\n",
      "Epoch [32/40] Batch [100/279] Loss: 0.1784\n",
      "Epoch [32/40] Batch [200/279] Loss: 0.2755\n",
      "Epoch [32/40]\n",
      "Train Loss: 0.2845 | Train Acc: 87.94%\n",
      "Val Loss: 0.2976 | Val Acc: 87.03%\n",
      "Saved new best model with validation accuracy: 87.03%\n",
      "Epoch [33/40] Batch [0/279] Loss: 0.2135\n",
      "Epoch [33/40] Batch [100/279] Loss: 0.1864\n",
      "Epoch [33/40] Batch [200/279] Loss: 0.6249\n",
      "Epoch [33/40]\n",
      "Train Loss: 0.2799 | Train Acc: 87.76%\n",
      "Val Loss: 0.3011 | Val Acc: 86.98%\n",
      "Epoch [34/40] Batch [0/279] Loss: 0.0373\n",
      "Epoch [34/40] Batch [100/279] Loss: 0.5694\n",
      "Epoch [34/40] Batch [200/279] Loss: 0.3815\n",
      "Epoch [34/40]\n",
      "Train Loss: 0.2797 | Train Acc: 87.88%\n",
      "Val Loss: 0.3270 | Val Acc: 85.86%\n",
      "Epoch [35/40] Batch [0/279] Loss: 0.1938\n",
      "Epoch [35/40] Batch [100/279] Loss: 0.4502\n",
      "Epoch [35/40] Batch [200/279] Loss: 0.2980\n",
      "Epoch [35/40]\n",
      "Train Loss: 0.2776 | Train Acc: 87.75%\n",
      "Val Loss: 0.3642 | Val Acc: 83.84%\n",
      "Epoch [36/40] Batch [0/279] Loss: 0.2600\n",
      "Epoch [36/40] Batch [100/279] Loss: 0.2192\n",
      "Epoch [36/40] Batch [200/279] Loss: 0.2114\n",
      "Epoch [36/40]\n",
      "Train Loss: 0.2814 | Train Acc: 87.95%\n",
      "Val Loss: 0.3117 | Val Acc: 87.34%\n",
      "Saved new best model with validation accuracy: 87.34%\n",
      "Epoch [37/40] Batch [0/279] Loss: 0.2218\n",
      "Epoch [37/40] Batch [100/279] Loss: 0.3009\n",
      "Epoch [37/40] Batch [200/279] Loss: 0.1648\n",
      "Epoch [37/40]\n",
      "Train Loss: 0.2665 | Train Acc: 88.64%\n",
      "Val Loss: 0.3016 | Val Acc: 86.71%\n",
      "Epoch [38/40] Batch [0/279] Loss: 0.2670\n",
      "Epoch [38/40] Batch [100/279] Loss: 0.1761\n",
      "Epoch [38/40] Batch [200/279] Loss: 0.4859\n",
      "Epoch [38/40]\n",
      "Train Loss: 0.2593 | Train Acc: 88.99%\n",
      "Val Loss: 0.3140 | Val Acc: 86.89%\n",
      "Epoch [39/40] Batch [0/279] Loss: 0.2047\n",
      "Epoch [39/40] Batch [100/279] Loss: 0.2270\n",
      "Epoch [39/40] Batch [200/279] Loss: 0.3370\n",
      "Epoch [39/40]\n",
      "Train Loss: 0.2333 | Train Acc: 90.20%\n",
      "Val Loss: 0.2838 | Val Acc: 88.51%\n",
      "Saved new best model with validation accuracy: 88.51%\n",
      "Epoch [40/40] Batch [0/279] Loss: 0.1137\n",
      "Epoch [40/40] Batch [100/279] Loss: 0.3215\n",
      "Epoch [40/40] Batch [200/279] Loss: 0.1613\n",
      "Epoch [40/40]\n",
      "Train Loss: 0.2280 | Train Acc: 90.41%\n",
      "Val Loss: 0.2816 | Val Acc: 88.38%\n",
      "\n",
      "TOTAL Final Evaluation Results:\n",
      "Mean F1 Score: 0.8812\n",
      "Accuracy: 0.8838\n",
      "\n",
      "UCI-HAR Dataset Evaluation:\n",
      "F1 Score: 0.9128\n",
      "Accuracy: 0.9136\n",
      "\n",
      "USC-HAD Dataset Evaluation:\n",
      "F1 Score: 0.8447\n",
      "Accuracy: 0.8595\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
